#!/usr/bin/env python3
"""
Test del cuentacuentos con dependencias reales
"""
import json
import sys
import time

sys.path.append('/home/ubuntu/cuenteria/src')

from llm_client import get_llm_client

def test_con_dependencias_reales():
    print("="*70)
    print("üß™ TEST CUENTACUENTOS CON DEPENDENCIAS REALES")
    print("="*70)
    
    client = get_llm_client()
    client.timeout = 300  # 5 minutos
    
    # Cargar el prompt real del sistema
    with open('flujo/v2/agentes/03_cuentacuentos.json', 'r') as f:
        agent_config = json.load(f)
    system_prompt = agent_config['content']
    
    # Cargar dependencias reales
    try:
        with open('runs/test-v2-emilia-1756440401/01_director.json', 'r') as f:
            director = json.load(f)
        with open('runs/test-v2-emilia-1756440401/02_psicoeducador.json', 'r') as f:
            psico = json.load(f)
        print("‚úì Dependencias reales cargadas")
    except:
        print("‚ö†Ô∏è Usando dependencias m√≠nimas de prueba")
        director = {
            "leitmotiv": "¬°Tic, tac, la canci√≥n del pastel!",
            "beat_sheet": [{"pagina": i, "objetivo": f"Obj {i}", 
                          "conflicto": f"Conf {i}" if i<10 else None,
                          "resolucion": f"Res {i}" if i==10 else None,
                          "emocion": "alegr√≠a", 
                          "imagen_nuclear": f"Img {i}"} for i in range(1,11)]
        }
        psico = {"mapa_psico_narrativo": {str(i): {"micro_habilidades": ["hab1"]} for i in range(1,11)}}
    
    # Construir prompt del usuario con diferentes tama√±os
    versiones = [
        ("M√çNIMO", "Genera el cuento. Usa el leitmotiv en p√°ginas 2, 5 y 10."),
        ("REDUCIDO", f"Usa este beat sheet:\n{json.dumps(director['beat_sheet'], ensure_ascii=False)}\nLeitmotiv: {director['leitmotiv']}"),
        ("COMPLETO", f"=== DIRECTOR ===\n{json.dumps(director, ensure_ascii=False, indent=2)}\n\n=== PSICOEDUCADOR ===\n{json.dumps(psico, ensure_ascii=False, indent=2)}")
    ]
    
    print(f"\nüìä Tama√±o del system prompt: {len(system_prompt)} chars")
    
    for nombre, user_prompt in versiones:
        print(f"\n{'='*50}")
        print(f"üîß Probando versi√≥n: {nombre}")
        print(f"   User prompt: {len(user_prompt)} chars")
        print(f"   Total: {len(system_prompt) + len(user_prompt)} chars")
        print(f"   Tokens aprox: {(len(system_prompt) + len(user_prompt)) // 4}")
        
        # Probar con diferentes max_tokens
        for max_tokens in [4000, 6000, 8000]:
            print(f"\n   ‚Üí max_tokens={max_tokens}")
            
            try:
                start = time.time()
                result = client.generate(
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                    temperature=0.7,
                    max_tokens=max_tokens
                )
                elapsed = time.time() - start
                
                print(f"   ‚úÖ √âXITO en {elapsed:.1f}s")
                
                # Verificar estructura
                if isinstance(result, str):
                    try:
                        result = json.loads(result)
                    except:
                        print(f"      ‚ö†Ô∏è No es JSON v√°lido")
                        continue
                
                if 'paginas_texto' in result:
                    print(f"      ‚úì {len(result['paginas_texto'])} p√°ginas generadas")
                    
                    # Guardar resultado exitoso
                    filename = f'cuentacuentos_{nombre}_{max_tokens}.json'
                    with open(filename, 'w') as f:
                        json.dump(result, f, ensure_ascii=False, indent=2)
                    print(f"      üíæ Guardado: {filename}")
                    
                    # Si funciona, no probar m√°s tokens
                    break
                else:
                    print(f"      ‚ö†Ô∏è Estructura incorrecta")
                    
            except ValueError as ve:
                elapsed = time.time() - start
                if "no gener√≥ contenido" in str(ve):
                    print(f"   ‚ùå Modelo devolvi√≥ vac√≠o en {elapsed:.1f}s")
                else:
                    print(f"   ‚ùå Error: {ve}")
            except Exception as e:
                print(f"   ‚ùå Error: {str(e)[:100]}")
    
    print("\n" + "="*70)
    print("üìä AN√ÅLISIS FINAL")
    print("="*70)
    
    import os
    exitosos = [f for f in os.listdir('.') if f.startswith('cuentacuentos_') and f.endswith('.json')]
    
    if exitosos:
        print(f"\n‚úÖ Configuraciones exitosas: {len(exitosos)}")
        for f in exitosos:
            size = os.path.getsize(f)
            print(f"   - {f} ({size} bytes)")
            
        # Analizar el m√°s exitoso
        with open(exitosos[0], 'r') as f:
            data = json.load(f)
            if 'paginas_texto' in data:
                total_chars = sum(len(v) for v in data['paginas_texto'].values())
                print(f"\nüìù An√°lisis del exitoso:")
                print(f"   - Total caracteres generados: {total_chars}")
                print(f"   - Promedio por p√°gina: {total_chars//10}")
    else:
        print("\n‚ùå Ninguna configuraci√≥n funcion√≥ con dependencias reales")
        print("\nüîç DIAGN√ìSTICO:")
        print("1. El prompt simple (512 chars) funciona ‚úì")
        print("2. El prompt con dependencias reales probablemente excede el contexto")
        print("3. Soluciones posibles:")
        print("   - Reducir el tama√±o de las dependencias")
        print("   - Resumir beat_sheet y mapa_psico")
        print("   - Dividir en sub-tareas (5 p√°ginas cada una)")
        print("   - Usar un prompt m√°s conciso")

if __name__ == "__main__":
    test_con_dependencias_reales()